2025-01-24 14:47:34.743 | INFO     | __main__:main:46 - Starting server, serving from directory /iopsstor/scratch/cscs/tkerimog/open_clip/mixtera_server
2025-01-24 14:47:34.745 | DEBUG    | mixtera.core.client.mixtera_client:__init__:138 - Initialized current mixture id to -1.
2025-01-24 14:47:34.746 | INFO     | mixtera.core.datacollection.mixtera_data_collection:_load_db_from_disk:76 - Loading database from /iopsstor/scratch/cscs/tkerimog/open_clip/mixtera_server/mixtera.duckdb
2025-01-24 14:47:34.769 | INFO     | mixtera.core.datacollection.mixtera_data_collection:_load_db_from_disk:78 - Database loaded.
2025-01-24 14:47:34.782 | INFO     | mixtera.core.datacollection.mixtera_data_collection:_vacuum:123 - Vacuuming the DuckDB.
2025-01-24 14:47:34.783 | INFO     | mixtera.core.datacollection.mixtera_data_collection:_vacuum:125 - Vacuumd.
2025-01-24 14:47:34.783 | DEBUG    | mixtera.core.query.query_cache:__init__:18 - Initializing QueryCache at /iopsstor/scratch/cscs/tkerimog/open_clip/mixtera_server/querycache
2025-01-24 14:47:34.793 | INFO     | mixtera.network.server.server:_run_async:377 - Serving MixteraServer on ('172.28.11.220', 12345)
0: slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
0: slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
0: 2025-01-24,14:47:58 | INFO | Running with a single process. Device cuda.
0: 2025-01-24,14:47:58 | INFO | Loaded ViT-B-32 model config.
0: 2025-01-24,14:48:00 | INFO | Model:
0: 2025-01-24,14:48:00 | INFO | CLIP(
0:   (visual): VisionTransformer(
0:     (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
0:     (patch_dropout): Identity()
0:     (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
0:     (transformer): Transformer(
0:       (resblocks): ModuleList(
0:         (0-11): 12 x ResidualAttentionBlock(
0:           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
0:           (attn): MultiheadAttention(
0:             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
0:           )
0:           (ls_1): Identity()
0:           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
0:           (mlp): Sequential(
0:             (c_fc): Linear(in_features=768, out_features=3072, bias=True)
0:             (gelu): GELU(approximate='none')
0:             (c_proj): Linear(in_features=3072, out_features=768, bias=True)
0:           )
0:           (ls_2): Identity()
0:         )
0:       )
0:     )
0:     (ln_post): LayerNorm((768,), eps=1e-05, elementwise_a
0: ffine=True)
0:   )
0:   (transformer): Transformer(
0:     (resblocks): ModuleList(
0:       (0-11): 12 x ResidualAttentionBlock(
0:         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
0:         (attn): MultiheadAttention(
0:           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
0:         )
0:         (ls_1): Identity()
0:         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
0:         (mlp): Sequential(
0:           (c_fc): Linear(in_features=512, out_features=2048, bias=True)
0:           (gelu): GELU(approximate='none')
0:           (c_proj): Linear(in_features=2048, out_features=512, bias=True)
0:         )
0:         (ls_2): Identity()
0:       )
0:     )
0:   )
0:   (token_embedding): Embedding(49408, 512)
0:   (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
0: )
0: 2025-01-24,14:48:00 | INFO | Params:
0: 2025-01-24,14:48:00 | INFO |   accum_freq: 1
0: 2025-01-24,14:48:00 | INFO |   aug_cfg: {}
0: 2025-01-24,14:48:00 | INFO |   batch_size: 256
0: 2025-01-24,14:48:00 | INFO |   beta1: 0.9
0: 2025-01-24,14:48:00 | INFO |   beta2: 0.98
0: 2025-01-24,14:48:00 | INFO |   cache_dir: None
0: 2025-01-24,14:48:00 | INFO |   checkpoint_path: ./logs/2025_01_24-14_47_58-model_ViT-B-32-lr_0.0005-b_256-j_4-p_amp/checkpoints
0: 2025-01-24,14:48:00 | INFO |   coca_caption_loss_weight: 2.0
0: 2025-01-24,14:48:00 | INFO |   coca_contrastive_loss_weight: 1.0
0: 2025-01-24,14:48:00 | INFO |   copy_codebase: False
0: 2025-01-24,14:48:00 | INFO |   csv_caption_key: title
0: 2025-01-24,14:48:00 | INFO |   csv_img_key: filepath
0: 2025-01-24,14:48:00 | INFO |   csv_separator: 	
0: 2025-01-24,14:48:00 | INFO |   dataset_resampled: False
0: 2025-01-24,14:48:00 | INFO |   dataset_type: mixtera_webdataset
0: 2025-01-24,14:48:00 | INFO |   ddp_static_graph: False
0: 2025-01-24,14:48:00 | INFO |   debug: False
0: 2025-01-24,14:48:00 | INFO |   delete_previous_checkpoint: False
0: 2025-01-24,14:48:00 | INFO |   device: cuda
0: 2025-01-24,14:48:00 | INFO |   dist_backend: None
0: 2025-01-24,14:48:00 | INFO |   dist_url: None
0: 2025-01-24,14:48:00 | INFO |   distill: False
0: 2025-01-24,14:48:00 | INFO |   distill_model: None
0: 2025-01-24,14:48:00 | INFO |   distill_pretrained: None
0: 2025-01-24,14:48:00 | INFO |   distributed: False
0: 2025-01-24,14:48:00 | INFO |   epochs: 5
0: 2025-01-24,14:48:00 | INFO |   epochs_cooldown: None
0: 2025-01-24,14:48:00 | INFO |   eps: 1e-06
0: 2025-01-24,14:48:00 | INFO |   force_custom_text: False
0: 2025-01-24,14:48:00 | INFO |   force_image_size: None
0: 2025-01-24,14:48:00 | INFO |   force_patch_dropout: None
0: 2025-01-24,14:48:00 | INFO |   force_quick_gelu: False
0: 2025-01-24,14:48:00 | INFO |   gather_with_grad: True
0: 2025-01-24,14:48:00 | INFO |   grad_checkpointing: False
0: 2025-01-24,14:48:00 | INFO |   grad_clip_norm: None
0: 2025-01-24,14:48:00 | INFO |   horovod: False
0: 2025-01-24,14:48:00 | INFO |   image_interpolation: None
0: 2025-01-24,14:48:00 | INFO |   image_mean: None
0: 2025-01-24,14:48:00 | INFO |   image_resize_mode: None
0: 2025-01-24,14:48:00 | INFO |   image_std: None
0: 2025-01-24,14:48:00 | INFO |   imagenet_v2: None
0: 2025-01-24,14:48:00 | INFO |   imagenet_val: None
0: 2025-01-24,14:48:00 | INFO |   local_loss: True
0: 2025-01-24,14:48:00 | INFO |   local_rank: 0
0: 2025-01-24,14:48:00 | INFO |   lock_image: False
0: 2025-01-24,14:48:00 | INFO |   lock_image_freeze_bn_stats: False
0: 2025-01-24,14:48:00 | INFO |   lock_image_unlocked_groups: 0
0: 2025-01-24,14:48:00 | INFO |   lock_text: False
0: 2025-01-24,14:48:00 | INFO |   lock_text_freeze_layer_norm: False
0: 2025-01-24,14:48:00 | INFO |   lock_text_unlocked_layers: 0
0: 2025-01-24,14:48:00 | INFO |   log_every_n_steps: 100
0: 2025-01-24,14:48:00 | INFO |   log_level: 20
0: 2025-01-24,14:48:00 | INFO |   log_local: False
0: 2025-01-24,14:48:00 | INFO |   log_path: ./logs/2025_01_24-14_47_58-model_ViT-B-32-lr_0.0005-b_256-j_4-p_amp/out.log
0: 2025-01-24,14:48:00 | INFO |   logs: ./logs/
0: 2025-01-24,14:48:00 | INFO |   loss_dist_impl: None
0: 2025-01-24,14:48:00 | INFO |   lr: 0.0005
0: 2025-01-24,14:48:00 | INFO |   lr_cooldown_end: 0.0
0: 2025-01-24,14:48:00 | INFO |   lr_cooldown_power: 1.0
0: 2025-01-24,14:48:00 | INFO |   lr_scheduler: cosine
0: 2025-01-24,14:48:00 | INFO |   model: ViT-B-32
0: 2025-01-24,14:48:00 | INFO |   momentum: None
0: 2025-01-24,14:48:00 | INFO |   name: 2025_01_24-14_47_58-model_ViT-B-32-lr_0.0005-b_256-j_4-p_amp
0: 2025-01-24,14:48:00 | INFO |   no_set_device_rank: False
0: 2025-01-24,14:48:00 | INFO |   opt: adamw
0: 2025-01-24,14:48:00 | INFO |   precision: amp
0: 2025-01-24,14:48:00 | INFO |   pretrained: 
0: 2025-01-24,14:48:00 | INFO |   pretrained_image: False
0: 2025-01-24,14:48:00 | INFO |   rank: 0
0: 2025-01-24,14:48:00 | INFO |   remote_sync: None
0: 2025-01-24,14:48:00 | INFO |   remote_sync_frequency: 300
0: 2025-01-24,14:48:00 | INFO |   remote_sync_protocol: s3
0: 2025-01-24,14:48:00 | INFO |   report_to: wandb
0: 2025-01-24,14:48:00 | INFO |   resume: None
0: 2025-01-24,14:48:00 | INFO |   save_frequency: 1
0: 2025-01-24,14:48:00 | INFO |   save_most_recent: False
0: 2025-01-24,14:48:00 | INFO |   seed: 0
0: 2025-01-24,14:48:00 | INFO |   siglip: False
0: 2025-01-24,14:48:00 | INFO |   skip_scheduler: False
0: 2025-01-24,14:48:00 | INFO |   tensorboard: False
0: 2025-01-24,14:48:00 | INFO |   tensorboard_path: 
0: 2025-01-24,14:48:00 | INFO |   torchcompile: False
0: 2025-01-24,14:48:00 | INFO |   torchscript: False
0: 2025-01-24,14:48:00 | INFO |   trace: False
0: 2025-01-24,14:48:00 | INFO |   train_data: /iopsstor/scratch/cscs/tkerimog/datasets/cc12m-wds/cc12m-train-{0000..2175}.tar
0: 2025-01-24,14:48:00 | INFO |   train_data_upsampling_factors: None
0: 2025-01-24,14:48:00 | INFO |   train_num_samples: 100000
0: 2025-01-24,14:48:00 | INFO |   use_bn_sync: False
0: 2025-01-24,14:48:00 | INFO |   use_bnb_linear: None
0: 2025-01-24,14:48:00 | INFO |   val_data: None
0: 2025-01-24,14:48:00 | INFO |   val_frequency: 1
0: 2025-01-24,14:48:00 | INFO |   val_num_samples: None
0: 2025-01-24,14:48:00 | INFO |   wandb: True
0: 2025-01-24,14:48:00 | INFO |   wandb_notes: 
0: 2025-01-24,14:48:00 | INFO |   wandb_project_name: open-clip
0: 2025-01-24,14:48:00 | INFO |   warmup: 10000
0: 2025-01-24,14:48:00 | INFO |   wd: 0.2
0: 2025-01-24,14:48:00 | INFO |   workers: 4
0: 2025-01-24,14:48:00 | INFO |   world_size: 1
0: 2025-01-24,14:48:00 | INFO |   zeroshot_frequency: 2
0: 2025-01-24,14:48:00 | INFO | Created AdamW (adamw) optimizer: lr: 0.0005, betas: (0.9, 0.98), eps: 1e-06, weight_decay: 0.2, amsgrad: False, foreach: None, maximize: False, capturable: False, differentiable: False, fused: None
0: 2025-01-24,14:48:00 | INFO | Creating Mixtera dataset with 1 data parallel groups.
0: ┬ T4: <class 'mixtera.core.query.mixture.arbitrary_mixture.ArbitraryMixture'>
0: └ # T4 [68 B]
0: ┬ D2: <dict object at 0x4004a23773c0>
0: └ # D2 [37 B]
0: ┬ T4: <class 'mixtera.core.query.query.Query'>
2025-01-24 14:48:00.768 | DEBUG    | mixtera.network.server.server:_register_query:61 - Received register query request
0: └ # T4 [37 B]
0: ┬ D2: <dict object at 0x4004a2377440>
0: ├┬ T4: <class 'mixtera.core.query.query_plan.QueryPlan'>
0: │└ # T4 [46 B]
0: ├┬ D2: <dict object at 0x40021d17fcc0>
0: │├┬ T4: <class 'mixtera.core.query.operators.select.Select'>
2025-01-24 14:48:00.768 | DEBUG    | mixtera.network.server.server:_register_query:63 - mixture = {"mixture": "arbitrary_mixture", "chunk_size": 1024}
0: ││└ # T4 [49 B]
0: │├┬ D2: <dict object at 0x4004a2377480>
0: ││└ # D2 [32 B]
0: │└ # D2 [95 B]
0: └ # D2 [217 B]
2025-01-24 14:48:00.768 | DEBUG    | mixtera.network.server.server:_register_query:74 - Received query = select<>([]). Executing it.
2025-01-24 14:48:00.779 | DEBUG    | mixtera.core.query.query_cache:get_queryresults_if_cached:97 - Returning results from cache!
2025-01-24 14:48:00.779 | INFO     | mixtera.core.query.query_result:from_cache:655 - Loading QueryResult from cache.
2025-01-24 14:48:00.780 | DEBUG    | mixtera.core.query.query_result:from_cache:661 - Loaded pickable attributes.
2025-01-24 14:48:00.782 | DEBUG    | mixtera.core.query.query_result:from_cache:667 - Loaded dillable attributes.
2025-01-24 14:48:00.782 | DEBUG    | mixtera.core.query.query_result:from_cache:679 - Instantiated QueryResult from pickle/dill.
2025-01-24 14:48:00.783 | DEBUG    | mixtera.core.query.query_result:from_cache:690 - Instantiated non-pickable attributes.
2025-01-24 14:48:00.791 | INFO     | mixtera.utils.utils:deserialize_chunker_index:364 - Deserializing all files in parallel (using 284 cores).
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.
In addition, using fork() with Python in general is a recipe for mysterious
deadlocks and crashes.

The most likely reason you are seeing this error is because you are using the
multiprocessing module on Linux, which uses fork() by default. This will be
fixed in Python 3.14. Until then, you want to use the "spawn" context instead.

See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.

  self.pid = os.fork()
Deserializing Files (using 284 cores):   0%|          | 0/2176 [00:00<?, ?it/s]Deserializing Files (using 284 cores):  56%|█████▌    | 1215/2176 [00:00<00:00, 12111.12it/s]Deserializing Files (using 284 cores): 100%|██████████| 2176/2176 [00:00<00:00, 12163.15it/s]
Merging loaded results:   0%|          | 0/2176 [00:00<?, ?it/s]Merging loaded results: 100%|██████████| 2176/2176 [00:00<00:00, 1611371.03it/s]
2025-01-24 14:48:02.155 | DEBUG    | mixtera.core.query.query_result:from_cache:694 - Loaded chunker index.
2025-01-24 14:48:02.260 | DEBUG    | mixtera.core.query.query_result:_update_key_id_map:110 - Updated key-id-map:
{dataset:CC12M: 0}

2025-01-24 14:48:02.260 | DEBUG    | mixtera.core.query.chunk_distributor:__init__:53 - [75840/75840] Instantiating ChunkDistributor for job mixtera_openclip_20250124_144731
2025-01-24 14:48:02.261 | INFO     | mixtera.core.client.local.local_stub:_register_query:166 - Registered query select<>([]) for job mixtera_openclip_20250124_144731, with mixture {"mixture": "arbitrary_mixture", "chunk_size": 1024}
2025-01-24 14:48:02.261 | DEBUG    | mixtera.network.server.server:_register_query:77 - Registered query with success = True and executed it.
0: 2025-01-24,14:48:02 | INFO | Mixtera loader initialized, num_samples: 100352, num_batches: 392
0: wandb: Currently logged in as: tkerimoglu (ethz-easl). Use `wandb login --relogin` to force relogin
0: wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
0: wandb: Tracking run with wandb version 0.19.4
0: wandb: Run data is saved locally in /workspace/wandb/run-20250124_144802-2025_01_24-14_47_58-model_ViT-B-32-lr_0.0005-b_256-j_4-p_amp
0: wandb: Run `wandb offline` to turn off syncing.
0: wandb: Syncing run 2025_01_24-14_47_58-model_ViT-B-32-lr_0.0005-b_256-j_4-p_amp
0: wandb: ⭐️ View project at https://wandb.ai/ethz-easl/open-clip
0: wandb: 🚀 View run at https://wandb.ai/ethz-easl/open-clip/runs/2025_01_24-14_47_58-model_ViT-B-32-lr_0.0005-b_256-j_4-p_amp
0: 2025-01-24,14:48:03 | INFO | Start epoch 0
0: /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.
0: In addition, using fork() with Python in general is a recipe for mysterious
0: deadlocks and crashes.
0: 
0: The most likely reason you are seeing this error is because you are using the
0: multiprocessing module on Linux, which uses fork() by default. This will be
0: fixed in Python 3.14. Until then, you want to use the "spawn" context instead.
0: 
0: See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.
0: 
0:   self.pid = os.fork()
2025-01-24 14:48:03.859 | DEBUG    | mixtera.core.query.query_result:_chunk_generator:419 - Obtained new None mixture.
2025-01-24 14:48:03.860 | DEBUG    | mixtera.core.query.query_result:_persist_mixture_log:119 - Persisting mixture log to disk...
2025-01-24 14:48:03.862 | DEBUG    | mixtera.core.query.query_result:_persist_mixture_log:132 - Mixture log persisted.
0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:818: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:670.)
0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
0: 2025-01-24,14:48:08 | INFO | Train Epoch: 0 [   256/100352 (0%)] Data (t): 3.204 Batch (t): 5.169, 49.5268/s, 49.5268/s/gpu LR: 0.000000 Logit Scale: 14.286 Contrastive_loss: 5.5930 (5.5930) Loss: 5.5930 (5.5930)
0: /usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 8. 
0:   warnings.warn(str(msg))
0: 2025-01-24,14:48:40 | INFO | Train Epoch: 0 [ 25856/100352 (26%)] Data (t): 0.236 Batch (t): 0.320, 2935.52/s, 2935.52/s/gpu LR: 0.000005 Logit Scale: 14.285 Contrastive_loss: 5.5203 (5.5566) Loss: 5.5203 (5.5566)
0: 2025-01-24,14:49:13 | INFO | Train Epoch: 0 [ 51456/100352 (51%)] Data (t): 0.244 Batch (t): 0.328, 2867.83/s, 2867.83/s/gpu LR: 0.000010 Logit Scale: 14.283 Contrastive_loss: 5.4276 (5.5136) Loss: 5.4276 (5.5136)
0: 2025-01-24,14:49:46 | INFO | Train Epoch: 0 [ 77056/100352 (77%)] Data (t): 0.247 Batch (t): 0.331, 2951.60/s, 2951.60/s/gpu LR: 0.000015 Logit Scale: 14.280 Contrastive_loss: 5.3335 (5.4686) Loss: 5.3335 (5.4686)
0: 2025-01-24,14:50:15 | INFO | Train Epoch: 0 [100352/100352 (100%)] Data (t): 0.226 Batch (t): 0.310, 2963.84/s, 2963.84/s/gpu LR: 0.000020 Logit Scale: 14.279 Contrastive_loss: 5.2947 (5.4338) Loss: 5.2947 (5.4338)
0: 2025-01-24,14:50:17 | INFO | Start epoch 1
0: Traceback (most recent call last):
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip-mixtera/src/open_clip_train/main.py", line 558, in <module>
0:     main(sys.argv[1:])
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip-mixtera/src/open_clip_train/main.py", line 484, in main
0:     train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip-mixtera/src/open_clip_train/train.py", line 85, in train_one_epoch
0:     for i, batch in enumerate(dataloader):
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/pipeline.py", line 70, in iterator
0:     yield from self.iterator1()
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 701, in __next__
0:     data = self._next_data()
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
0:     return self._process_data(data)
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
0:     data.reraise()
0:   File "/usr/local/lib/python3.10/dist-packages/torch/_utils.py", line 719, in reraise
0:     raise exception
0: AssertionError: Caught AssertionError in DataLoader worker process 0.
0: Original Traceback (most recent call last):
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
0:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 42, in fetch
0:     data = next(self.dataset_iter)
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/pipeline.py", line 70, in iterator
0:     yield from self.iterator1()
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 487, in _batched
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 397, in _to_tuple
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webda
0: taset/filters.py", line 372, in _map_dict
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 322, in _rename
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 285, in _decode
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 250, in _select
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/mixtera/mixtera/torch/mixtera_torch_dataset.py", line 247, in __iter__
0:     assert self._comp_shm is not None and self._status_shm is not None, "SharedMemory objects are None."
0: AssertionError: SharedMemory objects are None.
0: 
0: Traceback (most recent call last):
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip-mixtera/src/open_clip_train/main.py", line 558, in <module>
0:     main(sys.argv[1:])
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip-mixtera/src/open_clip_train/main.py", line 484, in main
0:     train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip-mixtera/src/open_clip_train/train.py", line 85, in train_one_epoch
0:     for i, batch in enumerate(dataloader):
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/pipeline.py", line 70, in iterator
0:     yield from self.iterator1()
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 701, in __next__
0:     data = self._next_data()
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
0:     return self._process_data(data)
0:   F
0: ile "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
0:     data.reraise()
0:   File "/usr/local/lib/python3.10/dist-packages/torch/_utils.py", line 719, in reraise
0:     raise exception
0: AssertionError: Caught AssertionError in DataLoader worker process 0.
0: Original Traceback (most recent call last):
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
0:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
0:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 42, in fetch
0:     data = next(self.dataset_iter)
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/pipeline.py", line 70, in iterator
0:     yield from self.iterator1()
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 487, in _batched
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs
0: /tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 397, in _to_tuple
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 372, in _map_dict
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 322, in _rename
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 285, in _decode
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/open_clip_env/lib/python3.10/site-packages/webdataset/filters.py", line 250, in _select
0:     for sample in data:
0:   File "/iopsstor/scratch/cscs/tkerimog/open_clip/mixtera/mixtera/torch/mixtera_torch_dataset.py", line 247, in __iter__
0:     assert self._comp_shm is not None and self._status_shm is not None, "SharedMemory objects are None."
0: AssertionError
0: : SharedMemory objects are None.
0: 
0: I0124 14:50:20.055000 76778 torch/_dynamo/utils.py:399] TorchDynamo compilation metrics:
0: I0124 14:50:20.055000 76778 torch/_dynamo/utils.py:399] Function    Runtimes (s)
0: I0124 14:50:20.055000 76778 torch/_dynamo/utils.py:399] ----------  --------------
0: I0124 14:50:20.057000 76778 torch/_subclasses/fake_tensor.py:2435] FakeTensor cache stats:
0: I0124 14:50:20.057000 76778 torch/_subclasses/fake_tensor.py:2436]   cache_hits: 0
0: I0124 14:50:20.058000 76778 torch/_subclasses/fake_tensor.py:2437]   cache_misses: 0
srun: error: nid005059: task 0: Exited with exit code 1
srun: Terminating StepId=109472.0
